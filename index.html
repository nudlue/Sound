<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8" />
    <title>Sound Classification Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>

    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
        }
        #status {
            font-size: 20px;
            font-weight: bold;
            margin-bottom: 20px;
        }
        button {
            font-size: 18px;
            padding: 12px 20px;
            margin-right: 10px;
        }
        #resultTable {
            margin-top: 20px;
            border-collapse: collapse;
        }
        #resultTable td, #resultTable th {
            border: 1px solid #aaa;
            padding: 10px 20px;
        }
    </style>
</head>

<body>

<h1>ğŸ”Š ì˜¤ë””ì˜¤ ë¶„ë¥˜ ë°ëª¨</h1>

<button onclick="startRecording()">ë…¹ìŒ ì‹œì‘</button>
<button onclick="stopRecording()">ë…¹ìŒ ì¢…ë£Œ</button>

<p id="status">ìƒíƒœ: ëŒ€ê¸° ì¤‘</p>

<table id="resultTable">
    <thead>
        <tr>
            <th>í´ë˜ìŠ¤</th>
            <th>í™•ë¥  (%)</th>
        </tr>
    </thead>
    <tbody></tbody>
</table>

<script>
let model;
let recorder;
let audioChunks = [];
let isRecording = false;

// ëª¨ë¸ ë¡œë“œ
async function loadModel() {
    const modelURL = "./model.json";
    const metadataURL = "./metadata.json";

    model = await speechCommands.create(
        'BROWSER_FFT',
        undefined,
        modelURL,
        metadataURL
    );

    await model.ensureModelLoaded();
    console.log("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ");
}
loadModel();

// ë…¹ìŒ ì‹œì‘
async function startRecording() {
    if (isRecording) return;
    isRecording = true;

    document.getElementById("status").innerText = "ìƒíƒœ: ğŸ™ ë…¹ìŒ ì¤‘â€¦";

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    recorder = new MediaRecorder(stream);

    audioChunks = [];

    recorder.ondataavailable = e => audioChunks.push(e.data);
    recorder.start();
}

// ë…¹ìŒ ì¢…ë£Œ
async function stopRecording() {
    if (!isRecording) return;
    isRecording = false;

    document.getElementById("status").innerText = "ìƒíƒœ: ì²˜ë¦¬ ì¤‘â€¦";

    recorder.stop();

    recorder.onstop = async () => {
        const blob = new Blob(audioChunks, { type: "audio/wav" });
        const arrayBuffer = await blob.arrayBuffer();
        const audioBuffer = await decodeAudioData(arrayBuffer);

        await classifyAudio(audioBuffer);
    };
}

// ArrayBuffer â†’ AudioBuffer
function decodeAudioData(arrayBuffer) {
    return new Promise((resolve) => {
        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        audioCtx.decodeAudioData(arrayBuffer, resolve);
    });
}

// ëª¨ë¸ ë¶„ì„
async function classifyAudio(audioBuffer) {
    const input = audioBuffer.getChannelData(0);

    // Teachable Machine Audio ì…ë ¥ì— ë§ê²Œ ë³€í™˜
    const fft = model.recognize(input);

    const prediction = await model.recognize(input);

    updateTable(prediction);
    document.getElementById("status").innerText = "ìƒíƒœ: ì™„ë£Œ";
}

// ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
function updateTable(prediction) {
    const tbody = document.querySelector("#resultTable tbody");
    tbody.innerHTML = "";

    const labels = model.wordLabels();

    prediction.scores.forEach((score, i) => {
        const tr = document.createElement("tr");
        tr.innerHTML = `
            <td>${labels[i]}</td>
            <td>${(score * 100).toFixed(2)}%</td>
        `;
        tbody.appendChild(tr);
    });
}
</script>
</body>
</html>